# ─────────────────────────
# App / Server
# ─────────────────────────
APP_NAME=Smart Assistant
APP_SECRET=CHANGE_ME
DEBUG=false
HOST=0.0.0.0
PORT=8000
APP_BASE_URL=http://127.0.0.1:8000

# ─────────────────────────
# AWS / S3
# ─────────────────────────

AWS_REGION=eu-west-3
#AWS_ACCESS_KEY_ID=CHANGE_ME
#AWS_SECRET_ACCESS_KEY=CHANGE_ME
# Si tu utilises un profile local (~/.aws/credentials) laisse les clés vides et mets le nom du profil :
#AWS_PROFILE=default

S3_BUCKET=smart-assistant-bucket
# Préfixe global optionnel (laisse vide si inutile)
S3_PREFIX=

# ─────────────────────────
# Caches & offline
# ─────────────────────────
# Racine de cache locale (Windows)
SA_CACHE_DIR=C:\tmp\sa_cache

# Cache HF local (même offline)
HF_HOME=C:\tmp\transformers_cache
HUGGINGFACE_HUB_CACHE=C:\tmp\transformers_cache

# Couper le réseau côté HF (recommandé si tout est en S3/locaux)


HF_HUB_DISABLE_SYMLINKS_WARNING=1
TOKENIZERS_PARALLELISM=false

PT_QG_S3_PREFIX=s3://smart-assistant-bucket/models/qg/pretrained_questions_model/checkpoints_flan_large/
PT_QG_CACHE_DIR=C:\tmp\sa_cache\qg_pt_flan
PT_QG_MODEL=google/flan-t5-large
PT_QG_HF_MODEL=google/flan-t5-large
ENABLE_QA_FILTER=false

QG_MAX_CHARS=20000
QG_PT_MAX_TOKENS=800
QG_PT_OVERLAP=120
# Activer les contraintes 'force_words_ids' (déconseillé si ta version HF plante)
QG_ENABLE_FORCE_WORDS=false


QG_GEN_NUM_BEAMS=4
QG_GEN_MAX_NEW_TOKENS=128
QG_GEN_NO_REPEAT_NGRAM_SIZE=4
QG_GEN_REPETITION_PENALTY=1.15
QG_GEN_DO_SAMPLE=false
QG_GEN_LENGTH_PENALTY=0.9

HF_HUB_OFFLINE=1
TRANSFORMERS_OFFLINE=1


# ─────────────────────────
# SUMMARY — From-Scratch (FS / HiBERT)
# (Conservé pour plus tard; non utilisé si tu testes PT d’abord)
# ─────────────────────────
SUM_FS_CHECKPOINT_S3=s3://smart-assistant-bucket/models/summary/checkpoints/hibert_dapt_best.pth
# Soit un dossier de tokenizer complet…
SUM_FS_TOKENIZER_DIR=
# …ou les deux fichiers séparés :
SUM_FS_TOKENIZER_MERGES_S3=s3://smart-assistant-bucket/models/summary/tokenizer/merges.txt
SUM_FS_TOKENIZER_VOCAB_S3=s3://smart-assistant-bucket/models/summary/tokenizer/vocab.json

# Fenêtres FS (si/qd activé)
SUMMARY_FS_MAX_TOKENS=900
SUMMARY_FS_OVERLAP=150

# ─────────────────────────
# SUMMARY — Pré-entraîné (PT / mBART50 fine-tuné)
# ─────────────────────────
# ─────────────────────────
# QUESTIONS — PT (FLAN-T5) via S3 → cache local
# ─────────────────────────


# Fallback HF (si S3 absent/incomplet ET offline désactivé)
PT_QG_MODEL=google/flan-t5-large
PT_QG_HF_MODEL=google/flan-t5-large

# QA filter (optionnel ; nécessite réseau pour charger SQuAD2 si pas en cache)
ENABLE_QA_FILTER=false

# Fenêtres / budgets QG
QG_MAX_CHARS=20000
QG_PT_MAX_TOKENS=800
QG_PT_OVERLAP=120

# Génération QG
QG_GEN_NUM_BEAMS=4
QG_GEN_MAX_NEW_TOKENS=128
QG_GEN_NO_REPEAT_NGRAM_SIZE=4
QG_GEN_REPETITION_PENALTY=1.15
QG_GEN_DO_SAMPLE=false
QG_GEN_LENGTH_PENALTY=0.9

# Offline (déjà dans ton .env ; garde-les si tu veux 100% cache)
HF_HUB_OFFLINE=1
TRANSFORMERS_OFFLINE=1


# Ignorer generation_config.json (si early_stopping: null, etc.)
PT_SUMMARY_IGNORE_GEN_CONFIG=true
# Préférence device (la pipeline bascule en CPU si CUDA indispo)
PT_SUMMARY_DEVICE_PREFERENCE=cuda
# Fallback HF (non utilisé si S3 présent + local_files_only)


# Fenêtres / limites PT


SUMMARY_MAX_CHARS=20000

# Guardrails génération PT (défauts sûrs)
SUMMARY_GEN_NUM_BEAMS=4
SUMMARY_GEN_EARLY_STOPPING=true
SUMMARY_GEN_NO_REPEAT_NGRAM_SIZE=4
SUMMARY_SAFETY_MARGIN=64
SUMMARY_SHOW_PROGRESS_EVERY=1

# Qualité / feedback
SUMMARY_FEEDBACK_MIN_QUALITY=0.45
SUMMARY_FEEDBACK_BIN_CHARS=4000

# ─────────────────────────
# Traduction (routing FR/EN)
# ─────────────────────────
ENABLE_OFFLINE_TRANSLATION=true
TRANSLATION_SRC=fr
TRANSLATION_TGT=en

# Sélection Top-K (plus de chunks)

SUMMARY_TOPK_K_MAX=3
SUMMARY_TOPK_MIN_SCORE=0.55
SUMMARY_TOPK_SCORE_GAP=0.08
SUMMARY_TOPK_MODE=join           # join | map
SUMMARY_TOPK_SEP="\n\n---\n\n"
SUMMARY_TOPK_JOIN_BUDGET=1400
SUMMARY_TOPK_JOIN_PROMPT_ENABLE=true
SUMMARY_TOPK_JOIN_PROMPT=Write a concise executive summary synthesizing the sections below. Avoid repetition and keep punctuation clean.\n\n

# Chunking de base (on garde des chunks raisonnables pour en avoir davantage)
SUMMARY_CHUNK_TOKENS=900
SUMMARY_CHUNK_OVERLAP=150
SUMMARY_SAFETY_MARGIN=64
 # budget de tokens EN avant résumé (sécurité)
# --- Feedback pipeline (modèle S3 + répertoire TMP local) ---
SUMMARY_S3_MODEL_URI=s3://smart-assistant-bucket/models/summary/pretrained_summary/better_checkpoint/checkpoint_improved/checkpoint-25366/
SUMMARY_MODEL_TMP_DIR=/tmp/sa_models/summary
# (optionnel) obliger S3 (pas de fallback HF/local) :
REQUIRE_S3=0
# --- Feedback (intention-aware) ---
FEEDBACK_TOPK_K=2
FEEDBACK_HEADER_RADIUS=8
FEEDBACK_FALLBACK_MAX_PARAGRAPHS=8

# --- Heuristique contexte single-chunk ---


# --- Top-K défaut pour feedback ---


PT_SUMMARY_CACHE_DIR=C:\tmp\sa_cache\summary_en_model
PT_SUMMARY_S3_PREFIX=s3://smart-assistant-bucket/models/summary/pretrained_summary/better_checkpoint/checkpoint_improved/checkpoint-25366/
PT_SUMMARY_HF_MODEL=facebook/mbart-large-50
PT_DEVICE=cuda

# Traduction Marian (pas de S3 pour Marian)
TRANS_CACHE_DIR=C:\tmp\sa_cache\translators
TRANS_FR2EN_ID=Helsinki-NLP/opus-mt-fr-en
TRANS_EN2FR_ID=Helsinki-NLP/opus-mt-en-fr
TRANS_DEVICE=cpu
TRANS_MAX_LENGTH=480
HF_HUB_OFFLINE=1
TRANSFORMERS_OFFLINE=1
REQUIRE_LOCAL=1

# Budgets
SUMMARY_PT_CTX_MAX=1024
SUMMARY_CTX_SAFETY_RATIO=0.90
SUMMARY_PT_MAX_TOKENS=900
SUMMARY_PT_OVERLAP=150

# Top-K
SUMMARY_TOPK_ENABLE=true
SUMMARY_TOPK_K=2


SUMMARY_GEN_NUM_BEAM_GROUPS=2
SUMMARY_GEN_DIVERSITY_PENALTY=0.3
SUMMARY_GEN_ENCODER_NO_REPEAT_NGRAM_SIZE=3
SUMMARY_GEN_REPETITION_PENALTY=1.25
SUMMARY_GEN_LENGTH_PENALTY=1.05
SUMMARY_GEN_MIN_NEW_TOKENS=60
SUMMARY_GEN_MAX_NEW_TOKENS=224
SUMMARY_GEN_DO_SAMPLE=false

# Sélection Top-K


SUMMARY_TOPK_MIN_SCORE=0.55
SUMMARY_TOPK_SIM_WEIGHT=0.6
SUMMARY_TOPK_QLT_WEIGHT=0.4


# ==== Traduction ====
TRANSLATION_CHUNK_TOKENS=480
TRANSLATION_NUM_BEAMS=4
TRANSLATION_MAX_NEW=512
TRANSLATION_USE_SAFETENSORS_ONLY=true
# (Optionnel) Miroirs Marian sur S3 si tu veux 100% offline :
# MARIAN_FR_EN_S3_PREFIX=s3://smart-assistant-bucket/models/translation/opus-mt-fr-en/
# MARIAN_EN_FR_S3_PREFIX=s3://smart-assistant-bucket/models/translation/opus-mt-en-fr/
# (Optionnel) caches locaux dédiés :
# MARIAN_FR_EN_CACHE_DIR=C:\tmp\sa_cache\marian_fr_en
# MARIAN_EN_FR_CACHE_DIR=C:\tmp\sa_cache\marian_en_fr

# ─────────────────────────
# QUESTIONS — From-Scratch (FS)
# ─────────────────────────
QG_FS_CHECKPOINT_S3=s3://smart-assistant-bucket/models/qg/checkpoints/qg_transformer_best.pt
# Soit dossier :
QG_FS_TOKENIZER_DIR=
# …ou merges+vocab :
QG_FS_TOKENIZER_MERGES_S3=s3://smart-assistant-bucket/models/qg/tokenizer/qg_from_scratch_tokenizer/merges.txt
QG_FS_TOKENIZER_VOCAB_S3=s3://smart-assistant-bucket/models/qg/tokenizer/qg_from_scratch_tokenizer/vocab.json

# Fenêtres FS QG
QG_FS_MAX_TOKENS=800
QG_FS_OVERLAP=120

# ─────────────────────────
# QUESTIONS — Pré-entraîné (PT / FLAN-T5)
# ─────────────────────────
# Modèle HF par défaut (tu peux aussi mirrorer sur S3 avec PT_QG_S3_PREFIX)
PT_QG_MODEL=google/flan-t5-large
PT_QG_HF_MODEL=google/flan-t5-large

# Fenêtres PT QG
QG_PT_MAX_TOKENS=800
QG_PT_OVERLAP=120
QG_MAX_CHARS=20000
TRANSLATION_CHUNK_TOKENS=480
TRANSLATION_MAX_NEW=512
TRANSLATION_NUM_BEAMS=4
TRANSLATION_USE_SAFETENSORS_ONLY=true

# Génération QG
QG_GEN_NUM_BEAMS=4
QG_GEN_MAX_NEW_TOKENS=128
QG_GEN_NO_REPEAT_NGRAM_SIZE=4
QG_GEN_REPETITION_PENALTY=1.15
QG_GEN_DO_SAMPLE=false

# ─────────────────────────
# UI / Templates (FastAPI)
# ─────────────────────────
TEMPLATES_DIR=app/templates
STATIC_DIR=app/static
